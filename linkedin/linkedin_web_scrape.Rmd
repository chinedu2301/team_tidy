---
title: "Most Valuable Data Science Skills"
author: "Preston Peck"
date: "10/12/2021"
output: html_document
---

# Import
```{r message=FALSE}
library(dplyr)
library(rvest)
library(stringr)
library(XML)
library(RSelenium)
library(here)
library(keyring)
library(DBI)

source(here("scripts", "skills_extraction.R"))
source(here("scripts", "insert_esmi.R"))
```



```{r include=FALSE}
#####OLD: Skill parsers
#skillsKeywords <- c(
#  "Qualification",
#  "Requirement",
#  "Education",
#  "Soft Skills",
#  "Hard Skills",
#  "What You'll Need",
#  "What You Need",
#  "What It Takes"
#)
#
#skillsRegex <- skillsKeywords %>%
#  paste(collapse = "|")
#
#Lazy operator matches soonest instead of latest
#skillsRegex <- paste("(", skillsRegex, ").*?", sep = "")
#parseSkillsRegex <- paste(skillsRegex, "(\\1|\\.)", sep = "")
#notBeginsWithNewlineRegex <- "^[^\\\n]"
```


# Start server
```{r}
#####Used selenium  to ensure page load, consistency, and to view and understand the process
driver <- rsDriver(browser = "firefox", port = 4568L)
remDr <- driver$client
```

# Form search URL
```{r}
spaceCharacter <- "%20"
andCharacter <- "&"



site <- "LinkedIn"
baseUrl <- "https://www.linkedin.com/jobs/search/?"



keywordsKey <- "keywords="
keywordsValue <- c("data", "scientist") %>%
  paste(collapse = spaceCharacter)
keywordsKeyValue <- paste(keywordsKey, keywordsValue, sep = "")



startKey <- "start="
startValue <- 0
startKeyValue <- paste(startKey, startValue, sep = "")



queries <- paste(keywordsKeyValue, startKeyValue, sep = andCharacter)

searchUrl <- paste(baseUrl, queries, sep = "")
searchUrl
```



# Find job redirect links
```{r}
scrollToBottom <- function(driver, reps = 3) {
  for (i in 1:reps) {
    driver$
      findElement("css", "body")$
      sendKeysToElement(list(key = "end"))
    Sys.sleep(2)
  }
}
```

```{r}
liElement <- "li"
aElement <- "a"

hrefAttribute <- "href"

jobsRouteRegex = "\\/jobs\\/"


remDr$navigate(searchUrl)
scrollToBottom(remDr)

urls <- remDr$getPageSource()[[1]] %>%
  read_html %>%
  html_elements(liElement) %>%
  html_elements(aElement) %>%
  html_attr(hrefAttribute)

urls <- urls[grepl(jobsRouteRegex, urls)]
```



# Scrape
## Functions
```{r}
emptyTable <- function(cols = 1, rows = 1, colNames = NULL) {
  tibble <- data.frame(matrix(NA, ncol = cols, nrow = rows)) %>%
    as_tibble
  
  if (!is.null(colNames)) {
    colnames(tibble) <- colNames
  }
  
  return(tibble)
}
```

```{r}
extractField <- function(field) {
  field <- remDr$getPageSource()[[1]] %>%
    read_html %>%
    html_element(field) %>%
    html_text2
  
  return(field)
}
```

```{r}
extractEmployment <- function(employment) {
  field <- remDr$getPageSource()[[1]] %>%
    read_html %>%
    html_elements(employment)
  
  if(!is.na(field) && length(field) > 1) {
    field <- field[2] %>%
      html_text2
  } else {
    field <- ""
  }
  
  field
  return(field)
}
```

```{r}
extractSalary <- function(salary, min = TRUE) {
  field <- extractField(salary)
  
  if(!is.na(field) && field != '') {
    field <- ((field %>%
      str_split(" - "))[[1]])
    
    field <- field[ifelse(!min && length(field) > 1, 2, 1)]
  }
  
  return(field)
}
```

## Setup
```{r}
title <- 'h1[class="top-card-layout__title topcard__title"]'
company <- 'a[class="topcard__org-name-link topcard__flavor--black-link"]'
employment_type <- 'span[class="description__job-criteria-text description__job-criteria-text--criteria"]'
location <- 'span[class="topcard__flavor topcard__flavor--bullet"]'
details <- 'div[class="show-more-less-html__markup show-more-less-html__markup--clamp-after-5"]'
salary_range <- 'div[class="salary compensation__salary"]'

sampleUrl <- "https://www.linkedin.com/jobs/view/2724684312/?alternateChannel=search&refId=a%2Fh7xo#2jT4A%2FxfhHuH7AQ%3D%3D&trackingId=n8Yytqaa%2FLIpZ5e3bnOi7g%3D%3D"

columnNames <- c(
  "job_title",
  "company_name",
  "employment_type",
  "description",
  "min_salary",
  "max_salary",
  "state",
  "job_url",
  "original_source"
)

scrape <- emptyTable(length(columnNames), 1, columnNames)

max <- length(urls)
```

## Extract
```{r}
for (i in 1:max) {
  Sys.sleep(2)
  
  url <- urls[i]
  remDr$navigate(url)

  job_title <- title %>%
    extractField
  
  company_name <- company %>%
    extractField
  
  employment <- employment_type %>%
    extractEmployment
    
  description <- details %>%
    extractField
  
  minSalary <- salary_range %>%
    extractSalary
  
  maxSalary <- salary_range %>%
    extractSalary(min = FALSE)
  
  state <- location %>%
    extractField
    
  scrape <- scrape %>%
    add_row(
      job_title = job_title,
      company_name = company_name,
      employment_type = employment,
      description = description,
      min_salary = minSalary,
      max_salary = maxSalary,
      state = state,
      job_url = urls[i],
      original_source = site
    )
}

scrape <- scrape[-1,]
scrape[-4]
```

# Analyze
```{r job-skills}
emsiClientIdKey <- "EMSI_CLIENT_ID"
emsiSecretKey <- "EMSI_SECRET"
scope <- "emsi_open"

confidence = "0.4"

client_id <- key_get(emsiClientIdKey)
secret <- key_get(emsiSecretKey)



access_token <- client_id %>%
  get_token(secret, scope)

all_skills_df <- scrape %>%
  get_dataset_skills(confidence, access_token)

all_skills_df

all_skills_df %>%
  write.csv("all_skills_df_batch.csv", row.names = FALSE)
```

# Insert
```{r}
host <- "TEAMTIDY_DB_HOST"
user <- "TEAMTIDY_DB_USER"
password <- "TEAMTIDY_DB_PASS"
dbname <- "data_science_jobs"



db_connection <- dbConnect(RMariaDB::MariaDB(),
                           host = Sys.getenv(host),
                           user = Sys.getenv(user),
                           password = Sys.getenv(password),
                           dbname = dbname)

insert_esmi_data(all_skills_df, db_connection)
```